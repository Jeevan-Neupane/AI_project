{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeZ0SDCtj0Cq",
        "outputId": "e8c92e87-9e1b-4ecf-fcea-80f5a8657bc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load MNIST Dataset\n",
        "# transform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: (x > 0.5).float())])  # Binarize images\n",
        "# train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "# test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# RBM Class\n",
        "class RBM(nn.Module):\n",
        "    def __init__(self, num_visible, num_hidden):\n",
        "        super(RBM, self).__init__()\n",
        "        self.num_visible = num_visible\n",
        "        self.num_hidden = num_hidden\n",
        "        self.W = nn.Parameter(torch.randn(num_hidden, num_visible) * 0.01)  # Weights\n",
        "        self.v_bias = nn.Parameter(torch.zeros(num_visible))  # Visible bias\n",
        "        self.h_bias = nn.Parameter(torch.zeros(num_hidden))  # Hidden bias\n",
        "\n",
        "    def forward(self, v):\n",
        "        \"\"\"One Gibbs sampling step: v -> h -> v'\"\"\"\n",
        "        h_prob = torch.sigmoid(torch.matmul(v, self.W.T) + self.h_bias)  # P(h|v)\n",
        "        h_state = (torch.rand_like(h_prob) < h_prob).float()  # Sample h\n",
        "        v_prob = torch.sigmoid(torch.matmul(h_state, self.W) + self.v_bias)  # P(v|h)\n",
        "        v_state = (torch.rand_like(v_prob) < v_prob).float()  # Sample v\n",
        "        return v_prob, v_state\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        \"\"\"Energy function for Contrastive Divergence.\"\"\"\n",
        "        # Term 1: Visible bias term (v^T * b_v)\n",
        "        vb_term = torch.matmul(v, self.v_bias)\n",
        "\n",
        "        # Term 2: Hidden term (sum over log(1 + exp(v^T * W_j + b_h_j)))\n",
        "        hidden_term = torch.sum(\n",
        "            torch.log(1 + torch.exp(torch.matmul(v, self.W.T) + self.h_bias)),\n",
        "            dim=1\n",
        "        )\n",
        "\n",
        "        # Free energy: F(v) = -vb_term - hidden_term\n",
        "        return -vb_term - hidden_term\n",
        "\n",
        "    def train_rbm(self, train_loader, lr=0.001, epochs=50):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=lr)  # Use Adam instead of SGD\n",
        "        loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            for batch, (data, _) in enumerate(train_loader):\n",
        "                v0 = data.view(-1, 28*28).to(device)  # Flatten images & move to GPU\n",
        "                v1_prob, v1_state = self.forward(v0)  # Gibbs sampling\n",
        "\n",
        "                # Compute gradients using Contrastive Divergence (CD-1)\n",
        "                loss = torch.mean(self.free_energy(v0)) - torch.mean(self.free_energy(v1_state))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            loss_history.append(avg_loss)\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # Visualize Weights & Reconstructions\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                self.visualize_weights()\n",
        "                self.visualize_reconstruction(v0, v1_prob)\n",
        "\n",
        "        # Plot loss curve\n",
        "        self.plot_loss(loss_history)\n",
        "\n",
        "    def plot_loss(self, loss_history):\n",
        "        \"\"\"Plot training loss curve.\"\"\"\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        plt.plot(loss_history, label=\"Loss\")\n",
        "        plt.xlabel(\"Epochs\")\n",
        "        plt.ylabel(\"Free Energy Loss\")\n",
        "        plt.title(\"RBM Training Loss Curve\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_weights(self, num_images=16):\n",
        "        \"\"\"Plot learned features (weights).\"\"\"\n",
        "        fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
        "        for i, ax in enumerate(axes.flatten()):\n",
        "            if i >= num_images:\n",
        "                break\n",
        "            weight_img = self.W[i].detach().cpu().view(28, 28)\n",
        "            ax.imshow(weight_img, cmap=\"gray\")\n",
        "            ax.axis(\"off\")\n",
        "        plt.suptitle(\"RBM Learned Features\")\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_reconstruction(self, original, reconstructed, num_images=10):\n",
        "        \"\"\"Visualize original and reconstructed images.\"\"\"\n",
        "        fig, axes = plt.subplots(2, num_images, figsize=(15, 3))\n",
        "        for i in range(num_images):\n",
        "            # Original\n",
        "            axes[0, i].imshow(original[i].detach().cpu().view(28, 28), cmap=\"gray\")\n",
        "            axes[0, i].axis(\"off\")\n",
        "\n",
        "            # Reconstructed\n",
        "            axes[1, i].imshow(reconstructed[i].detach().cpu().view(28, 28), cmap=\"gray\")\n",
        "            axes[1, i].axis(\"off\")\n",
        "\n",
        "        axes[0, 0].set_title(\"Original Images\")\n",
        "        axes[1, 0].set_title(\"Reconstructed Images\")\n",
        "        plt.show()\n",
        "\n",
        "    def evaluate(self, test_loader):\n",
        "        \"\"\"Evaluate RBM on unseen test data using MSE, SSIM, and PSNR.\"\"\"\n",
        "        mse_total, ssim_total, psnr_total, count = 0, 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch, (data, _) in enumerate(test_loader):\n",
        "                v0 = data.view(-1, 28*28).to(device)  # Flatten images & move to GPU\n",
        "                v1_prob, _ = self.forward(v0)  # Reconstruct images\n",
        "\n",
        "                # Convert to numpy for metric calculations\n",
        "                original_np = v0.cpu().numpy()\n",
        "                reconstructed_np = v1_prob.cpu().numpy()\n",
        "\n",
        "                # Compute MSE, SSIM, PSNR\n",
        "                for i in range(original_np.shape[0]):\n",
        "                    mse = np.mean((original_np[i] - reconstructed_np[i])**2)\n",
        "                    ssim_score = ssim(original_np[i].reshape(28, 28), reconstructed_np[i].reshape(28, 28), data_range=1)\n",
        "                    psnr_score = psnr(original_np[i], reconstructed_np[i], data_range=1)\n",
        "\n",
        "                    mse_total += mse\n",
        "                    ssim_total += ssim_score\n",
        "                    psnr_total += psnr_score\n",
        "                    count += 1\n",
        "\n",
        "\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n--- RBM Evaluation on Unseen Data ---\")\n",
        "        print(f\"Mean Squared Error (MSE): {mse_total / count:.5f}\")\n",
        "        print(f\"Structural Similarity Index (SSIM): {ssim_total / count:.5f}\")\n",
        "        print(f\"Peak Signal-to-Noise Ratio (PSNR): {psnr_total / count:.5f}\")\n",
        "\n",
        "        # Visualize some reconstructions\n",
        "        self.visualize_reconstruction(v0, v1_prob)\n",
        "\n",
        "    def extract_features(self, data_loader):\n",
        "        \"\"\"Extract hidden layer features for classification\"\"\"\n",
        "        features, labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch, (data, targets) in enumerate(data_loader):\n",
        "                v = data.view(-1, 28 * 28).to(device)\n",
        "                h_prob, _ = self.forward(v)  # Get hidden layer activations\n",
        "                features.append(h_prob.cpu().numpy())\n",
        "                labels.append(targets.cpu().numpy())\n",
        "\n",
        "        return np.vstack(features), np.hstack(labels)\n",
        "\n",
        "# # Train RBM\n",
        "# rbm = RBM(num_visible=28*28, num_hidden=256).to(device)  # 256 hidden neurons, move to GPU\n",
        "# rbm.train_rbm(train_loader, lr=0.001, epochs=50)\n",
        "\n",
        "# # Evaluate on Unseen Test Data\n",
        "# rbm.evaluate(test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewQuENtZj5z0",
        "outputId": "d1a787c9-4806-4fca-8301-074e73cb4bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Save the trained model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# torch.save(rbm.state_dict(), \"/content/drive/MyDrive/research/rbm_mnist.pth\")\n",
        "# print(\"✅ Model saved as rbm_mnist.pth\")\n",
        "\n",
        "# Load the saved model\n",
        "# rbm_loaded = RBM(num_visible=28*28, num_hidden=256).to(device)  # Initialize same architecture\n",
        "# rbm_loaded.load_state_dict(torch.load(\"/content/drive/MyDrive/research/rbm_mnist.pth\", map_location=torch.device('cpu')))\n",
        "# rbm_loaded.eval()  # Set to evaluation mode\n",
        "# print(\"✅ Model loaded successfully\")\n",
        "# rbm = rbm_loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElGu6Uonj7vx"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# # Extract features using the trained RBM\n",
        "# train_features, train_labels = rbm.extract_features(train_loader)\n",
        "# test_features, test_labels = rbm.extract_features(test_loader)\n",
        "\n",
        "# # Train SVM Classifier\n",
        "# classifier = SVC(kernel='rbf', C=1.0, gamma='scale')  # RBF kernel for better performance\n",
        "# classifier.fit(train_features, train_labels)\n",
        "\n",
        "# # Evaluate Model\n",
        "# predictions = classifier.predict(test_features)\n",
        "# accuracy = accuracy_score(test_labels, predictions)\n",
        "# print(f\"Classification Accuracy: {accuracy:.4f}\")\n",
        "# import joblib\n",
        "# joblib.dump(classifier, \"/content/drive/MyDrive/research/logistic_classifier.pkl\")\n",
        "\n",
        "# print(\"Models saved successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rb-14po7kU_t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import joblib\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Load the trained RBM and SVM model\n",
        "device = torch.device(\"cpu\")\n",
        "rbm_path = \"/content/drive/MyDrive/research/rbm_mnist.pth\"\n",
        "rbm = RBM(num_visible=28*28, num_hidden=256).to(device)  # Initialize same architecture\n",
        "rbm.load_state_dict(torch.load(rbm_path, map_location=device))\n",
        "rbm.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Load trained SVM classifier\n",
        "svm_path = \"/content/drive/MyDrive/research/svm_classifier.pkl\"\n",
        "svm_classifier = joblib.load(svm_path)\n",
        "\n",
        "# Define image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),  # Resize to MNIST size\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def predict_digit(image_path: str,rbm=rbm, svm_classifier=svm_classifier, transform=transform):\n",
        "    \"\"\"\n",
        "    Predicts the digit in the image using the trained RBM and SVM classifier.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the input image.\n",
        "\n",
        "    Returns:\n",
        "        int: Predicted digit label.\n",
        "    \"\"\"\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"L\")  # Convert to grayscale\n",
        "    image = transform(image)  # Apply preprocessing\n",
        "    image = image.view(-1, 28*28)  # Flatten image\n",
        "\n",
        "    # Extract features using the trained RBM\n",
        "    with torch.no_grad():\n",
        "        features,_ = rbm(image)  # Extract RBM features\n",
        "\n",
        "    # Convert features to numpy array for SVM input\n",
        "    # features = features.numpy().reshape(1, -1)  # Flatten the feature vector\n",
        "\n",
        "    # Predict with SVM classifier\n",
        "\n",
        "    predicted_label = svm_classifier.predict(features)[0]\n",
        "\n",
        "    return int(predicted_label)\n",
        "\n",
        "# # # Example usage\n",
        "# image_path = \"image.jpg\"  # Path to an image file\n",
        "# predicted_digit = predict_digit(image_path)\n",
        "# print(f\"Predicted Digit: {predicted_digit}\")\n",
        "# type((predicted_digit))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qEh_zG5WkYR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7419b6c2-c88e-44f9-f399-3f88c954da43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/132.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m794.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q fastapi uvicorn python-multipart torch torchvision pytorchvideo pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def download_image(url, save_path=None):\n",
        "    \"\"\"\n",
        "    Download an image from the given URL and save it to the specified path.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL of the image to download\n",
        "        save_path (str, optional): Path where the image should be saved.\n",
        "                                  If None, saves with original filename in current directory.\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the saved image file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Send request to get the image\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()  # Raise an exception for bad responses\n",
        "\n",
        "        # Extract filename from URL if save_path is not provided\n",
        "        if not save_path:\n",
        "            parsed_url = urlparse(url)\n",
        "            filename = os.path.basename(parsed_url.path)\n",
        "\n",
        "            # Use a default name if filename couldn't be determined\n",
        "            if not filename:\n",
        "                filename = \"image.jpg\"\n",
        "\n",
        "            save_path = filename\n",
        "\n",
        "        # Create directories if they don't exist\n",
        "        directory = os.path.dirname(save_path)\n",
        "        if directory and not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "\n",
        "        # Write the image to file\n",
        "        with open(save_path, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        return os.path.abspath(save_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading image: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "hNCb9oYTfnuB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB-Aw6RckZXu",
        "outputId": "e4c27b45-5379-47a2-d988-4d47b1b69943"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [181]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public_URL: https://3494-34-46-36-171.ngrok-free.app\n",
            "INFO:     2407:5200:405:1d7d:a152:1f97:7099:9392:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     2407:5200:405:1d7d:a152:1f97:7099:9392:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     2407:5200:405:1d7d:a152:1f97:7099:9392:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2407:5200:405:1d7d:a152:1f97:7099:9392:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Finished server process [181]\n"
          ]
        }
      ],
      "source": [
        "import uvicorn\n",
        "import torch\n",
        "import joblib\n",
        "import numpy as np\n",
        "from fastapi import FastAPI, File, UploadFile, Query\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import requests\n",
        "from pydantic import BaseModel\n",
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Allow all CORS requests\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "async def predict_digit_from_image(file: UploadFile = File(...)):\n",
        "    image_path = \"image.jpg\"\n",
        "\n",
        "    # Write the uploaded file content to the image path\n",
        "    with open(image_path, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "\n",
        "\n",
        "    print(image_path)\n",
        "    # Pass the file path to the predict_digit function\n",
        "    predicted_label = predict_digit(image_path)\n",
        "    print(f\"predicted {predicted_label}, {type(predicted_label)}\")\n",
        "    # Clean up the temp file after prediction\n",
        "\n",
        "    return {\"predicted_digit\": predicted_label}\n",
        "\n",
        "class URLInput(BaseModel):\n",
        "    url: str\n",
        "@app.post(\"/predict_from_url/\")\n",
        "async def predict_digit_from_url(image_url: URLInput):\n",
        "    # image_path = download_image(\"https://res.cloudinary.com/chatappjeevanneupane/image/upload/v1730944042/vp9z8mt1oushfsszpdvg.jpg\")\n",
        "    image_path = download_image(image_url.url)\n",
        "\n",
        "    predicted_label = predict_digit(image_path)\n",
        "    print(f\"predicted {predicted_label}, {type(predicted_label)}\")\n",
        "\n",
        "    return {\"predicted_digit\": predicted_label}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import os\n",
        "    os.environ[\"NGROK_AUTHTOKEN\"] = \"2rCbW45ffaTmVf03HbMluTUNCv1_4uD242hh56wg9SvHorrNR\"\n",
        "\n",
        "    ngrok_tunnel = ngrok.connect(8000)\n",
        "    print('Public_URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "    nest_asyncio.apply()\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}